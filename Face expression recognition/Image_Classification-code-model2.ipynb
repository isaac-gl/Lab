{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label(label_fl):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_fl, 'r') as f:\n",
    "        label1 = f.readlines()\n",
    "        label1 = [l.strip() for l in label1]\n",
    "    label2 = {}\n",
    "    count = 0\n",
    "    for label in label1:\n",
    "        label2[label] = count\n",
    "        count += 1\n",
    "    return label1, label2\n",
    "\n",
    "\n",
    "train_path = 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/train'\n",
    "labels_path = 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/labels.txt'\n",
    "test_path = 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/test'\n",
    "\n",
    "filenames = [file for file in glob.glob(train_path + '*/*')]  #get all filenames\n",
    "\n",
    "labels = [file.split('_')[-1].split('.')[0] for file in filenames ] #get label string name eg. AU1, AU2...\n",
    "\n",
    "label1, label2 = get_label(labels_path) \n",
    "\n",
    "labels = [label2[label] for label in labels if label in label2] #map label  name to id number eg  AU2 -> 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Major defines\n",
    "N_INPUTS  = 32*32*3\n",
    "N_CLASSES = 7\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"c = 0\\nfor i in filenames:\\n    if 'mpg' in i:\\n        c = c+1\\nprint(c)\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"c = 0\n",
    "for i in filenames:\n",
    "    if 'mpg' in i:\n",
    "        c = c+1\n",
    "print(c)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create input dataset and generate batches of data\n",
    "def dataset_distribution(filenames, labels):\n",
    "    img_str = tf.read_file(filenames)\n",
    "    img_d = tf.image.decode_png(img_str, channels=3)\n",
    "    \n",
    "    #image augmentation\n",
    "    float_image = tf.image.per_image_standardization(img_d)\n",
    "    img_d = tf.reshape(float_image , [-1])\n",
    "    return img_d, labels\n",
    "\n",
    "#Train data\n",
    "data = tf.data.Dataset.from_tensor_slices((filenames[0:1800], labels[0:1800]))\n",
    "data = data.map(dataset_distribution, num_parallel_calls = 3)\n",
    "data = data.batch(BATCH_SIZE)\n",
    "data = data.shuffle(buffer_size=200)    #random number: lower so that buffer filling happens faster\n",
    "iterator = data.make_initializable_iterator()\n",
    "train_batch = iterator.get_next()  #next_element is tensor of (img_train, y_train)\n",
    "\n",
    "# #validation data \n",
    "val = tf.data.Dataset.from_tensor_slices((filenames[1800:], labels[1800:]))   #validation batch\n",
    "val = val.map(dataset_distribution, num_parallel_calls = 3)\n",
    "val = val.batch(100)\n",
    "validation_iterator = val.make_initializable_iterator()\n",
    "validation_batch = validation_iterator.get_next()\n",
    "\n",
    "\n",
    "\n",
    "#For test images\n",
    "def _parse_function_test(filenames):\n",
    "    img_str = tf.read_file(filenames)\n",
    "    img_d = tf.image.decode_png(img_str, channels=3)\n",
    "    \n",
    "    #Reserved for image augmentation\n",
    "    float_image = tf.image.per_image_standardization(img_d)\n",
    "    img_d = tf.reshape(float_image , [-1])  #flatten\n",
    "    \n",
    "    return img_d\n",
    "\n",
    "filenames_test = [file_t for file_t in glob.glob(test_path + '*/*')]  \n",
    "filenames_test.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "lst = []\n",
    "for i in range(len(filenames_test)):\n",
    "    splt = filenames_test[i].split(\"\\\\\")[2].split('.')[0]\n",
    "    lst.append(splt)\n",
    "\n",
    "#For test data\n",
    "data_test = tf.data.Dataset.from_tensor_slices(filenames_test)\n",
    "data_test = data_test.map(_parse_function_test)\n",
    "data_test = data_test.batch(311)\n",
    "test_iterator = data_test.make_initializable_iterator()\n",
    "test_batch = test_iterator.get_next() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_model(X, N_CLASSES, reuse, is_training):\n",
    "    \n",
    "    with tf.variable_scope('Conv', reuse = reuse): #to reuse weights and biases for testing\n",
    "        \n",
    "        input_layer = tf.reshape(X, [-1, 32,32,3])\n",
    "\n",
    "        conv0 = tf.layers.conv2d(\n",
    "          inputs=input_layer,\n",
    "          filters=32,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        batchnorm0 = tf.layers.batch_normalization(conv0)\n",
    "        \n",
    "        pool0 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm0,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = \"same\",\n",
    "            data_format='channels_last')\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(\n",
    "          inputs=pool0,\n",
    "          filters=64,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu) \n",
    "       \n",
    "        batchnorm1 = tf.layers.batch_normalization(conv1)\n",
    "\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm1,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = \"same\",\n",
    "            data_format='channels_last')\n",
    "          \n",
    "        dropout0 = tf.layers.dropout(\n",
    "            inputs = pool1,\n",
    "            rate=0.30,\n",
    "            training = is_training)\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=dropout0,\n",
    "            filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "        batchnorm2 = tf.layers.batch_normalization(conv2)\n",
    "        \n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm2,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding=\"same\",\n",
    "            data_format='channels_last')\n",
    "       \n",
    "        conv3 = tf.layers.conv2d(\n",
    "          inputs=pool2,\n",
    "          filters=256,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        batchnorm3 = tf.layers.batch_normalization(conv3)\n",
    "        \n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm3,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding=\"same\",\n",
    "            data_format='channels_last')\n",
    "        \n",
    "        dropout1 = tf.layers.dropout(\n",
    "            inputs = pool3,\n",
    "            rate=0.25,\n",
    "            training = is_training)\n",
    "\n",
    "        flatten = tf.layers.flatten(dropout1)\n",
    "\n",
    "        dense1 = tf.layers.dense(\n",
    "            inputs = flatten,\n",
    "            units = 1024,\n",
    "            activation= tf.nn.relu)\n",
    "\n",
    "        dense2 = tf.layers.dense(\n",
    "            inputs = dense1,\n",
    "            units = 512,\n",
    "            activation= tf.nn.relu)\n",
    "\n",
    "        dropout2 = tf.layers.dropout(\n",
    "            inputs = dense2,\n",
    "            rate=0.35,\n",
    "            training = is_training)\n",
    "        \n",
    "        dense3 = tf.layers.dense(\n",
    "            inputs = dropout2,\n",
    "            units = N_CLASSES)\n",
    "        \n",
    "        if is_training: last_layer = dense3     #using sparse cross entropy so no need to apply softmax here\n",
    "        else: last_layer = tf.nn.softmax(dense3)   #for inference\n",
    "\n",
    "        return last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = train_batch\n",
    "validation_X, validation_Y = validation_batch\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable = False, name='global_step')\n",
    "\n",
    "out_train = conv_model(X, N_CLASSES, reuse = False, is_training = True)\n",
    "\n",
    "#Cost function as softmax cross entropy\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits (labels = y, logits = out_train))\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost, global_step = global_step)\n",
    "\n",
    "#Train data accuracy\n",
    "out_test = conv_model(X, N_CLASSES, reuse=True, is_training=False)\n",
    "pred = tf.equal(tf.argmax(out_test, 1, output_type=tf.int32), y)\n",
    "acc_train = tf.reduce_mean(tf.cast(pred, tf.float32))\n",
    "\n",
    "#Validation data accuracy\n",
    "val_output = conv_model(validation_X, N_CLASSES, reuse=True, is_training=False)\n",
    "val_prediction = tf.equal(tf.argmax(val_output, 1, output_type=tf.int32), validation_Y)\n",
    "acc_val = tf.reduce_mean(tf.cast(val_prediction, tf.float32))\n",
    "\n",
    "#test data prediction\n",
    "out_test = conv_model(test_batch, N_CLASSES, reuse=True, is_training=False)\n",
    "test_pred = tf.argmax(out_test, 1 , output_type=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Conv_3/Softmax:0' shape=(?, 7) dtype=float32>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1    Loss: 2.4648    Train Accuracy: 0.03    Val Accuracy: 0.50    Time: 15.29\n",
      "Epoch 2    Loss: 0.8137    Train Accuracy: 0.91    Val Accuracy: 0.39    Time: 12.59\n",
      "Epoch 3    Loss: 0.7286    Train Accuracy: 1.00    Val Accuracy: 0.91    Time: 12.83\n",
      "Epoch 4    Loss: 0.1000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 14.47\n",
      "Epoch 5    Loss: 0.3808    Train Accuracy: 0.75    Val Accuracy: 1.00    Time: 16.85\n",
      "Epoch 6    Loss: 0.9556    Train Accuracy: 0.75    Val Accuracy: 0.93    Time: 15.07\n",
      "Epoch 7    Loss: 1.5986    Train Accuracy: 0.75    Val Accuracy: 0.91    Time: 14.60\n",
      "Epoch 8    Loss: 0.0011    Train Accuracy: 1.00    Val Accuracy: 0.91    Time: 14.55\n",
      "Epoch 9    Loss: 0.3663    Train Accuracy: 1.00    Val Accuracy: 0.91    Time: 14.83\n",
      "Epoch 10    Loss: 0.7782    Train Accuracy: 1.00    Val Accuracy: 0.91    Time: 13.08\n",
      "Epoch 11    Loss: 0.9177    Train Accuracy: 0.62    Val Accuracy: 1.00    Time: 13.02\n",
      "Epoch 12    Loss: 0.6586    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.25\n",
      "Epoch 13    Loss: 0.3873    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.16\n",
      "Epoch 14    Loss: 1.2900    Train Accuracy: 0.06    Val Accuracy: 0.50    Time: 13.10\n",
      "Epoch 15    Loss: 0.6034    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.09\n",
      "Epoch 16    Loss: 0.2049    Train Accuracy: 0.88    Val Accuracy: 0.91    Time: 13.06\n",
      "Epoch 17    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.11\n",
      "Epoch 18    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.36\n",
      "Epoch 19    Loss: 0.6783    Train Accuracy: 0.88    Val Accuracy: 0.97    Time: 13.03\n",
      "Epoch 20    Loss: 0.1064    Train Accuracy: 0.91    Val Accuracy: 1.00    Time: 13.00\n",
      "Epoch 21    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.03\n",
      "Epoch 22    Loss: 0.1707    Train Accuracy: 0.88    Val Accuracy: 0.98    Time: 12.97\n",
      "Epoch 23    Loss: 0.0044    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.14\n",
      "Epoch 24    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.91    Time: 13.01\n",
      "Epoch 25    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.02\n",
      "Epoch 26    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.07\n",
      "Epoch 27    Loss: 0.0610    Train Accuracy: 1.00    Val Accuracy: 0.91    Time: 13.01\n",
      "Epoch 28    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.01\n",
      "Epoch 29    Loss: 0.0620    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.11\n",
      "Epoch 30    Loss: 0.4651    Train Accuracy: 1.00    Val Accuracy: 0.91    Time: 13.02\n",
      "Epoch 31    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.43\n",
      "Epoch 32    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.15\n",
      "Epoch 33    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.19\n",
      "Epoch 34    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.13\n",
      "Epoch 35    Loss: 0.0116    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.11\n",
      "Epoch 36    Loss: 0.0089    Train Accuracy: 1.00    Val Accuracy: 0.98    Time: 13.23\n",
      "Epoch 37    Loss: 0.2308    Train Accuracy: 0.97    Val Accuracy: 0.99    Time: 13.03\n",
      "Epoch 38    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 12.97\n",
      "Epoch 39    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 12.93\n",
      "Epoch 40    Loss: 0.0103    Train Accuracy: 0.97    Val Accuracy: 0.99    Time: 13.18\n",
      "Epoch 41    Loss: 0.0033    Train Accuracy: 1.00    Val Accuracy: 0.92    Time: 13.10\n",
      "Epoch 42    Loss: 0.0069    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.17\n",
      "Epoch 43    Loss: 0.0031    Train Accuracy: 1.00    Val Accuracy: 0.98    Time: 13.00\n",
      "Epoch 44    Loss: 0.0564    Train Accuracy: 0.97    Val Accuracy: 0.99    Time: 12.99\n",
      "Epoch 45    Loss: 0.0001    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.02\n",
      "Epoch 46    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.03\n",
      "Epoch 47    Loss: 0.0001    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.63\n",
      "Epoch 48    Loss: 0.0008    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.96\n",
      "Epoch 49    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 12.99\n",
      "Epoch 50    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.08\n",
      "Epoch 51    Loss: 0.1121    Train Accuracy: 0.97    Val Accuracy: 0.99    Time: 12.99\n",
      "Epoch 52    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.01\n",
      "Epoch 53    Loss: 0.0178    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.18\n",
      "Epoch 54    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.00\n",
      "Epoch 55    Loss: 0.0001    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.08\n",
      "Epoch 56    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.19\n",
      "Epoch 57    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.96\n",
      "Epoch 58    Loss: 0.0012    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.05\n",
      "Epoch 59    Loss: 0.0171    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.09\n",
      "Epoch 60    Loss: 0.0034    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.02\n",
      "Epoch 61    Loss: 0.0024    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.05\n",
      "Epoch 62    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.97\n",
      "Epoch 63    Loss: 0.0001    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.01\n",
      "Epoch 64    Loss: 0.0002    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.95\n",
      "Epoch 65    Loss: 0.0002    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.02\n",
      "Epoch 66    Loss: 0.0009    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.03\n",
      "Epoch 67    Loss: 0.0021    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.96\n",
      "Epoch 68    Loss: 0.0004    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.99\n",
      "Epoch 69    Loss: 0.0046    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.07\n",
      "Epoch 70    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.97\n",
      "Epoch 71    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.04\n",
      "Epoch 72    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.96\n",
      "Epoch 73    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.95\n",
      "Epoch 74    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.96\n",
      "Epoch 75    Loss: 0.0007    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.98\n",
      "Epoch 76    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.92\n",
      "Epoch 77    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.98\n",
      "Epoch 78    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.09\n",
      "Epoch 79    Loss: 0.1019    Train Accuracy: 0.97    Val Accuracy: 1.00    Time: 12.93\n",
      "Epoch 80    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.21\n",
      "Epoch 81    Loss: 0.0008    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.29\n",
      "Epoch 82    Loss: 1.1941    Train Accuracy: 0.91    Val Accuracy: 1.00    Time: 13.16\n",
      "Epoch 83    Loss: 0.5340    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 12.97\n",
      "Epoch 84    Loss: 0.0886    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.18\n",
      "Epoch 85    Loss: 0.1129    Train Accuracy: 0.97    Val Accuracy: 0.99    Time: 12.97\n",
      "Epoch 86    Loss: 0.0001    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 12.95\n",
      "Epoch 87    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.09\n",
      "Epoch 88    Loss: 0.0039    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.01\n",
      "Epoch 89    Loss: 0.0056    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 12.98\n",
      "Epoch 90    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 12.99\n",
      "Epoch 91    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.02\n",
      "Epoch 92    Loss: 0.0203    Train Accuracy: 1.00    Val Accuracy: 0.99    Time: 13.05\n",
      "Epoch 93    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.17\n",
      "Epoch 94    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.01\n",
      "Epoch 95    Loss: 0.0101    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.03\n",
      "Epoch 96    Loss: 0.0008    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97    Loss: 1.0944    Train Accuracy: 0.72    Val Accuracy: 0.99    Time: 13.72\n",
      "Epoch 98    Loss: 0.0851    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 2,171.54\n",
      "Epoch 99    Loss: 0.0000    Train Accuracy: 1.00    Val Accuracy: 1.00    Time: 13.69\n",
      "Done with train\n",
      " saved at C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/FC-model1/model-10947\n",
      "Write complete\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "tr_accuracy = []\n",
    "val_accuracy = []\n",
    "tr_loss = []\n",
    "val_loss = []\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)   #initialize variables\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    \n",
    "    eCount = 1\n",
    "        \n",
    "    while (eCount < NUM_EPOCHS):\n",
    "        stTime = time.time()\n",
    "        while True:\n",
    "            try:\n",
    "                sess.run(optimizer)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sess.run(iterator.initializer)\n",
    "                training_loss, training_acc = sess.run([cost, acc_train])\n",
    "                tr_accuracy.append(training_acc)\n",
    "                tr_loss.append(training_loss)\n",
    "                \n",
    "                sess.run(validation_iterator.initializer)\n",
    "                validation_acc = sess.run(acc_val)\n",
    "                validation_loss = sess.run(cost)\n",
    "                \n",
    "                tr_loss.append(validation_acc)\n",
    "                val_loss.append(validation_loss)\n",
    "                \n",
    "                print(\"Epoch {}    Loss: {:,.4f}    Train Accuracy: {:,.2f}    Val Accuracy: {:,.2f}    Time: {:,.2f}\"         \n",
    "                      .format(eCount, training_loss, training_acc, validation_acc, time.time() - stTime))\n",
    "                \n",
    "               \n",
    "                eCount += 1\n",
    "                break\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print ('\\nInterrupted at epoch %d' % eCount)\n",
    "            \n",
    "                eCount = NUM_EPOCHS + 1\n",
    "                break\n",
    "    print ('Done with train')\n",
    "    #Save the model\n",
    "    save_path = saver.save(sess, 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/FC-model1/model', global_step = global_step )\n",
    "    print (' saved at %s' % save_path)  \n",
    "    \n",
    "    sess.run(test_iterator.initializer)\n",
    "    test_predictions = sess.run(test_pred)\n",
    "    pred_csv = open('C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/final_res1'+str(eCount)+'.csv', 'w')\n",
    "    header = ['id','label']\n",
    "    with pred_csv:\n",
    "        writer = csv.writer(pred_csv)\n",
    "        writer.writerow((header[0], header[1]))\n",
    "        for count, row in enumerate(range(test_predictions.shape[0])):\n",
    "            writer.writerow((lst[count], label1[test_predictions[count]]))\n",
    "        print(\"Write complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Final result on test data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/final_res1200.csv')\n",
    "df['id'] = df['id'].str.split('_')\n",
    "a_count = df[df['id'].str[1]==df['label']].count()[0]\n",
    "total_test_Sample = 311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.318327974276528"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = (a_count/total_test_Sample)*100\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
