{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label(label_fl):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_fl, 'r') as f:\n",
    "        label1 = f.readlines()\n",
    "        label1 = [l.strip() for l in label1]\n",
    "    label2 = {}\n",
    "    count = 0\n",
    "    for label in label1:\n",
    "        label2[label] = count\n",
    "        count += 1\n",
    "    return label1, label2\n",
    "\n",
    "\n",
    "train_path = 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/FC-model3/train'\n",
    "labels_path = 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/FC-model3/labels.txt'\n",
    "test_path = 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/FC-model3/test'\n",
    "\n",
    "filenames = [file for file in glob.glob(train_path + '*/*')]  #get all filenames\n",
    "\n",
    "labels = [file.split('_')[-1].split('.')[0] for file in filenames ] #get label string name eg. AU1, AU2...\n",
    "\n",
    "label1, label2 = get_label(labels_path) \n",
    "\n",
    "labels = [label2[label] for label in labels if label in label2] #map label  name to id number eg  AU2 -> 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Major defines\n",
    "N_INPUTS  = 32*32*3\n",
    "N_CLASSES = 6\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"c = 0\\nfor i in filenames:\\n    if 'mpg' in i:\\n        c = c+1\\nprint(c)\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"c = 0\n",
    "for i in filenames:\n",
    "    if 'mpg' in i:\n",
    "        c = c+1\n",
    "print(c)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input dataset and generate batches of data\n",
    "def dataset_distribution(filenames, labels):\n",
    "    img_str = tf.read_file(filenames)\n",
    "    img_d = tf.image.decode_png(img_str, channels=3)\n",
    "    \n",
    "    #image augmentation\n",
    "    float_image = tf.image.per_image_standardization(img_d)\n",
    "    img_d = tf.reshape(float_image , [-1])\n",
    "    return img_d, labels\n",
    "\n",
    "#Train data\n",
    "data = tf.data.Dataset.from_tensor_slices((filenames[0:32000], labels[0:32000]))\n",
    "data = data.map(dataset_distribution, num_parallel_calls = 3)\n",
    "data = data.batch(BATCH_SIZE)\n",
    "data = data.shuffle(buffer_size=200)    #random number: lower so that buffer filling happens faster\n",
    "iterator = data.make_initializable_iterator()\n",
    "train_batch = iterator.get_next()  #next_element is tensor of (img_train, y_train)\n",
    "\n",
    "# #validation data \n",
    "val = tf.data.Dataset.from_tensor_slices((filenames[32000:], labels[32000:]))   #validation batch\n",
    "val = val.map(dataset_distribution, num_parallel_calls = 3)\n",
    "val = val.batch(2000)\n",
    "validation_iterator = val.make_initializable_iterator()\n",
    "validation_batch = validation_iterator.get_next()\n",
    "\n",
    "\n",
    "\n",
    "#For test images\n",
    "def _parse_function_test(filenames):\n",
    "    img_str = tf.read_file(filenames)\n",
    "    img_d = tf.image.decode_png(img_str, channels=3)\n",
    "    \n",
    "    #Reserved for image augmentation\n",
    "    float_image = tf.image.per_image_standardization(img_d)\n",
    "    img_d = tf.reshape(float_image , [-1])  #flatten\n",
    "    \n",
    "    return img_d\n",
    "\n",
    "filenames_test = [file_t for file_t in glob.glob(test_path + '*/*')]  \n",
    "filenames_test.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "lst = []\n",
    "for i in range(len(filenames_test)):\n",
    "    splt = filenames_test[i].split(\"\\\\\")[2].split('.')[0]\n",
    "    if splt != 'desktop':\n",
    "        lst.append(splt)\n",
    "\n",
    "#For test data\n",
    "data_test = tf.data.Dataset.from_tensor_slices(filenames_test[1:])\n",
    "data_test = data_test.map(_parse_function_test)\n",
    "data_test = data_test.batch(641)\n",
    "test_iterator = data_test.make_initializable_iterator()\n",
    "test_batch = test_iterator.get_next() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_model(X, N_CLASSES, reuse, is_training):\n",
    "    \n",
    "    with tf.variable_scope('Conv', reuse = reuse): #to reuse weights and biases for testing\n",
    "        \n",
    "        input_layer = tf.reshape(X, [-1, 32,32,3])\n",
    "\n",
    "        conv0 = tf.layers.conv2d(\n",
    "          inputs=input_layer,\n",
    "          filters=32,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        batchnorm0 = tf.layers.batch_normalization(conv0)\n",
    "        \n",
    "        pool0 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm0,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = \"same\",\n",
    "            data_format='channels_last')\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(\n",
    "          inputs=pool0,\n",
    "          filters=64,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu) \n",
    "       \n",
    "        batchnorm1 = tf.layers.batch_normalization(conv1)\n",
    "\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm1,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = \"same\",\n",
    "            data_format='channels_last')\n",
    "          \n",
    "        dropout0 = tf.layers.dropout(\n",
    "            inputs = pool1,\n",
    "            rate=0.30,\n",
    "            training = is_training)\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=dropout0,\n",
    "            filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "        batchnorm2 = tf.layers.batch_normalization(conv2)\n",
    "        \n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm2,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding=\"same\",\n",
    "            data_format='channels_last')\n",
    "       \n",
    "        conv3 = tf.layers.conv2d(\n",
    "          inputs=pool2,\n",
    "          filters=256,\n",
    "          kernel_size=[3, 3],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "        batchnorm3 = tf.layers.batch_normalization(conv3)\n",
    "        \n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            inputs = batchnorm3,\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding=\"same\",\n",
    "            data_format='channels_last')\n",
    "        \n",
    "        dropout1 = tf.layers.dropout(\n",
    "            inputs = pool3,\n",
    "            rate=0.25,\n",
    "            training = is_training)\n",
    "\n",
    "        flatten = tf.layers.flatten(dropout1)\n",
    "\n",
    "        dense1 = tf.layers.dense(\n",
    "            inputs = flatten,\n",
    "            units = 1024,\n",
    "            activation= tf.nn.relu)\n",
    "\n",
    "        dense2 = tf.layers.dense(\n",
    "            inputs = dense1,\n",
    "            units = 512,\n",
    "            activation= tf.nn.relu)\n",
    "\n",
    "        dropout2 = tf.layers.dropout(\n",
    "            inputs = dense2,\n",
    "            rate=0.35,\n",
    "            training = is_training)\n",
    "        \n",
    "        dense3 = tf.layers.dense(\n",
    "            inputs = dropout2,\n",
    "            units = N_CLASSES)\n",
    "        \n",
    "        if is_training: last_layer = dense3     #using sparse cross entropy so no need to apply softmax here\n",
    "        else: last_layer = tf.nn.softmax(dense3)   #for inference\n",
    "\n",
    "        return last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = train_batch\n",
    "validation_X, validation_Y = validation_batch\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable = False, name='global_step')\n",
    "\n",
    "out_train = conv_model(X, N_CLASSES, reuse = False, is_training = True)\n",
    "\n",
    "#Cost function as softmax cross entropy\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits (labels = y, logits = out_train))\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost, global_step = global_step)\n",
    "\n",
    "#Train data accuracy\n",
    "out_test = conv_model(X, N_CLASSES, reuse=True, is_training=False)\n",
    "pred = tf.equal(tf.argmax(out_test, 1, output_type=tf.int32), y)\n",
    "acc_train = tf.reduce_mean(tf.cast(pred, tf.float32))\n",
    "\n",
    "#Validation data accuracy\n",
    "val_output = conv_model(validation_X, N_CLASSES, reuse=True, is_training=False)\n",
    "val_prediction = tf.equal(tf.argmax(val_output, 1, output_type=tf.int32), validation_Y)\n",
    "acc_val = tf.reduce_mean(tf.cast(val_prediction, tf.float32))\n",
    "\n",
    "#test data prediction\n",
    "out_test = conv_model(test_batch, N_CLASSES, reuse=True, is_training=False)\n",
    "test_pred = tf.argmax(out_test, 1 , output_type=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Conv_3/Softmax:0' shape=(?, 6) dtype=float32>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1    Loss: 0.4890    Train Accuracy: 1.00    Val Accuracy: 0.84    Time: 452.54\n",
      "Epoch 2    Loss: 0.6935    Train Accuracy: 0.84    Val Accuracy: 0.84    Time: 450.06\n",
      "Epoch 3    Loss: 1.4989    Train Accuracy: 0.00    Val Accuracy: 0.84    Time: 454.91\n",
      "Epoch 4    Loss: 1.1196    Train Accuracy: 0.44    Val Accuracy: 0.84    Time: 405.10\n",
      "Epoch 5    Loss: 0.5793    Train Accuracy: 0.84    Val Accuracy: 0.82    Time: 239.58\n",
      "Epoch 6    Loss: 0.7799    Train Accuracy: 0.62    Val Accuracy: 0.82    Time: 264.96\n",
      "Epoch 7    Loss: 0.0465    Train Accuracy: 1.00    Val Accuracy: 0.82    Time: 247.61\n",
      "Epoch 8    Loss: 1.4872    Train Accuracy: 0.62    Val Accuracy: 0.83    Time: 232.90\n",
      "Epoch 9    Loss: 0.0868    Train Accuracy: 1.00    Val Accuracy: 0.82    Time: 230.47\n",
      "Epoch 10    Loss: 0.2109    Train Accuracy: 0.97    Val Accuracy: 0.82    Time: 229.33\n",
      "Epoch 11    Loss: 0.1305    Train Accuracy: 0.97    Val Accuracy: 0.82    Time: 229.34\n",
      "Epoch 12    Loss: 0.0178    Train Accuracy: 1.00    Val Accuracy: 0.83    Time: 289.55\n",
      "Epoch 13    Loss: 0.0539    Train Accuracy: 1.00    Val Accuracy: 0.81    Time: 229.98\n",
      "Epoch 14    Loss: 0.3715    Train Accuracy: 0.91    Val Accuracy: 0.82    Time: 231.11\n",
      "Epoch 15    Loss: 0.0423    Train Accuracy: 1.00    Val Accuracy: 0.81    Time: 230.02\n",
      "Epoch 16    Loss: 0.5042    Train Accuracy: 0.91    Val Accuracy: 0.83    Time: 230.06\n",
      "Epoch 17    Loss: 0.1224    Train Accuracy: 1.00    Val Accuracy: 0.82    Time: 230.55\n",
      "Epoch 18    Loss: 0.1713    Train Accuracy: 1.00    Val Accuracy: 0.83    Time: 415.59\n",
      "Epoch 19    Loss: 0.0408    Train Accuracy: 1.00    Val Accuracy: 0.82    Time: 460.32\n",
      "\n",
      "Interrupted at epoch 20\n",
      "Done with train\n",
      " saved at C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/FC-model3/FC-Model3save/model-18976\n",
      "Write complete\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 150\n",
    "tr_accuracy = []\n",
    "val_accuracy = []\n",
    "tr_loss = []\n",
    "val_loss = []\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)   #initialize variables\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    \n",
    "    eCount = 1\n",
    "        \n",
    "    while (eCount < NUM_EPOCHS):\n",
    "        stTime = time.time()\n",
    "        while True:\n",
    "            try:\n",
    "                sess.run(optimizer)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sess.run(iterator.initializer)\n",
    "                training_loss, training_acc = sess.run([cost, acc_train])\n",
    "                tr_accuracy.append(training_acc)\n",
    "                tr_loss.append(training_loss)\n",
    "                \n",
    "                sess.run(validation_iterator.initializer)\n",
    "                validation_acc = sess.run(acc_val)\n",
    "                validation_loss = sess.run(cost)\n",
    "                \n",
    "                tr_loss.append(validation_acc)\n",
    "                val_loss.append(validation_loss)\n",
    "                \n",
    "                print(\"Epoch {}    Loss: {:,.4f}    Train Accuracy: {:,.2f}    Val Accuracy: {:,.2f}    Time: {:,.2f}\"         \n",
    "                      .format(eCount, training_loss, training_acc, validation_acc, time.time() - stTime))\n",
    "                \n",
    "               \n",
    "                eCount += 1\n",
    "                break\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print ('\\nInterrupted at epoch %d' % eCount)\n",
    "            \n",
    "                eCount = NUM_EPOCHS + 1\n",
    "                break\n",
    "    print ('Done with train')\n",
    "    #Save the model\n",
    "    save_path = saver.save(sess, 'C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/FC-model3/FC-Model3save/model', global_step = global_step )\n",
    "    print (' saved at %s' % save_path) \n",
    "    \n",
    "    sess.run(test_iterator.initializer)\n",
    "    test_predictions = sess.run(test_pred)\n",
    "    pred_csv = open('C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/FC-model3/final_res1'+str(eCount)+'.csv', 'w')\n",
    "    header = ['id','label']\n",
    "    with pred_csv:\n",
    "        writer = csv.writer(pred_csv)\n",
    "        writer.writerow((header[0], header[1]))\n",
    "        for count, row in enumerate(range(test_predictions.shape[0])):\n",
    "            writer.writerow((lst[count], label1[test_predictions[count]]))\n",
    "        print(\"Write complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final result on test data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('C:/Users/Vijay/Isaac/Issac Video/Working_code_on_training_vedios/FC-model3/final_res131.csv')\n",
    "df['id'] = df['id'].str.split('_')\n",
    "a_count = df[df['id'].str[1]==df['label']].count()[0]\n",
    "total_test_Sample = 641"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.311733800350268"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = (a_count/total_test_Sample)*100\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
